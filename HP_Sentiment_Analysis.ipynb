{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5b400c2c",
      "metadata": {
        "id": "5b400c2c"
      },
      "source": [
        "<h1 style=\"background-color:#000000;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\"><span style=\"color:#FFFFFF\">Harry Potter Sentiment Analysis</span></h1>\n",
        "\n",
        "\n",
        "\n",
        "- [1. Libraries](#1)\n",
        "\n",
        "- [2. Loading Data](#2)\n",
        "\n",
        "- [3. Preprocessing](#3)\n",
        "    \n",
        "- [4. Exploratory Data Analysis](#4)\n",
        "    * [4.1 General Statistics](#4.1)\n",
        "    * [4.2 Character Appearance Frequency](#4.2)\n",
        "   \n",
        "- [5. Sentiment Analysis](#5)\n",
        "    * [5.1 Sentence](#5.1)\n",
        "    * [5.2 Chapter](#5.2)\n",
        "    * [5.3 Entire Series](#5.2)\n",
        "\n",
        "- [6. LDA Topic Modeling](#6)\n",
        "- [7. ](#7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf8caef6",
      "metadata": {
        "id": "cf8caef6"
      },
      "source": [
        "<a id=\"1\"></a>\n",
        "\n",
        "<h1 style=\"background-color:#740001;font-family:newtimeroman;font-size:225%;text-align:center;border-radius:15px 50px;\"><span style=\"color:#EEBA30\">Libraries</span></h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45daaceb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "45daaceb",
        "outputId": "fab31449-ddbc-4821-c3a5-8233b01f6ed6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b100982c42f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0menable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action = \"ignore\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import json\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import corpus\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "from collections import Counter\n",
        "import re\n",
        "from operator import itemgetter\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.summarization import summarize\n",
        "\n",
        "import string\n",
        "from string import punctuation\n",
        "import os\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8c2fa94",
      "metadata": {
        "id": "d8c2fa94"
      },
      "source": [
        "<a id=\"2\"></a>\n",
        "\n",
        "<h1 style=\"background-color:#740001;font-family:newtimeroman;font-size:225%;text-align:center;border-radius:15px 50px;\"><span style=\"color:#EEBA30\">Loading Data</span></h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7777d35b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "7777d35b",
        "outputId": "99fb4a2b-86a5-42f1-d383-a18b6433a4f7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f22f78c7aeed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load character list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcharacters_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"characters_list.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'characters_list.csv'"
          ]
        }
      ],
      "source": [
        "# Load character list\n",
        "characters_list=pd.read_csv(\"characters_list.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c0a22ec",
      "metadata": {
        "id": "1c0a22ec"
      },
      "outputs": [],
      "source": [
        "# Create a list for all books\n",
        "all_books = ['book_1.txt',\n",
        "             'book_2.txt',\n",
        "             'book_3.txt',\n",
        "             'book_4.txt',\n",
        "             'book_5.txt',\n",
        "             'book_6.txt',\n",
        "             'book_7.txt'\n",
        "]\n",
        "\n",
        "# Load books\n",
        "def load_book(book_path):\n",
        "    with open(book_path,encoding='utf8') as input_file:\n",
        "        return input_file.read()\n",
        "\n",
        "def load_all_books():\n",
        "    books = []\n",
        "    for book in all_books:\n",
        "        books.append(load_book(book))\n",
        "    return books\n",
        "\n",
        "book_1 = load_book(all_books[0])\n",
        "book_2 = load_book(all_books[1])\n",
        "book_3 = load_book(all_books[2])\n",
        "book_4 = load_book(all_books[3])\n",
        "book_5 = load_book(all_books[4])\n",
        "book_6 = load_book(all_books[5])\n",
        "book_7 = load_book(all_books[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d1137d1",
      "metadata": {
        "id": "7d1137d1"
      },
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "def prep(text):\n",
        "    text = re.sub(\"<[^>]*>\", \" \", text) # Remove HTML tags\n",
        "    text = re.sub(r\"\\\\w\\w\\w\\\\w\\w\\w\",\" \",text) # Remove redundant characters\n",
        "    text = \" \".join(text.split()) # Remove extra spaces, tabs, and new lines\n",
        "    return text\n",
        "\n",
        "book_1 = prep(book_1)\n",
        "book_2 = prep(book_2)\n",
        "book_3 = prep(book_3)\n",
        "book_4 = prep(book_4)\n",
        "book_5 = prep(book_5)\n",
        "book_6 = prep(book_6)\n",
        "book_7 = prep(book_7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c08bb85",
      "metadata": {
        "id": "6c08bb85"
      },
      "outputs": [],
      "source": [
        "# Load chapter names\n",
        "chapters_file = 'chapters.json'\n",
        "\n",
        "def load_chapter_names():\n",
        "    with open(chapters_file) as input_file:\n",
        "        chapters=json.load(input_file)\n",
        "    return chapters\n",
        "\n",
        "chapter_names = load_chapter_names()\n",
        "\n",
        "def load_chapters(book, chapter_names):\n",
        "    book_chapters = []\n",
        "    current_chapter = chapter_names[0]\n",
        "    current_chapter_location = len(current_chapter) + 1\n",
        "    for chapter in chapter_names[1:]:\n",
        "        # Find where next chapter starts\n",
        "        next_chapter_location = book.find(chapter)\n",
        "        # Get chapter text -- all text from the location of current chapter till the start of the next chapter\n",
        "        book_chapters.append(book[current_chapter_location:next_chapter_location])\n",
        "        # Move start pointer to the next chapter\n",
        "        current_chapter_location = next_chapter_location + len(chapter)\n",
        "        # Save next chapter name\n",
        "        current_chapter = chapter\n",
        "    book_chapters.append(book[current_chapter_location:])\n",
        "    return book_chapters\n",
        "\n",
        "def load_all_chapters():\n",
        "    book_chapters = []\n",
        "    chapter_names = load_chapter_names()\n",
        "    for book_number, book in enumerate(load_all_books()):\n",
        "        chapters = load_chapters(book, chapter_names[book_number])\n",
        "        book_chapters.append(chapters)\n",
        "    return book_chapters, chapter_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba73ec35",
      "metadata": {
        "id": "ba73ec35"
      },
      "outputs": [],
      "source": [
        "book_1_chapters, book_1_chapter_names = load_chapters(book_1, chapter_names[0]), chapter_names[0]\n",
        "book_2_chapters, book_2_chapter_names = load_chapters(book_2, chapter_names[1]), chapter_names[1]\n",
        "book_3_chapters, book_3_chapter_names = load_chapters(book_3, chapter_names[2]), chapter_names[2]\n",
        "book_4_chapters, book_4_chapter_names = load_chapters(book_4, chapter_names[3]), chapter_names[3]\n",
        "book_5_chapters, book_5_chapter_names = load_chapters(book_5, chapter_names[4]), chapter_names[4]\n",
        "book_6_chapters, book_6_chapter_names = load_chapters(book_6, chapter_names[5]), chapter_names[5]\n",
        "book_7_chapters, book_7_chapter_names = load_chapters(book_7, chapter_names[6]), chapter_names[6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a1466cf",
      "metadata": {
        "id": "7a1466cf"
      },
      "outputs": [],
      "source": [
        "# Tokenize the sentences\n",
        "def tokenizeSentence(book, sent_list):\n",
        "    for i in range(len(book)):\n",
        "        sentence = sent_tokenize(book[i])\n",
        "        sent_list += sentence\n",
        "\n",
        "book_1_sent = []\n",
        "tokenizeSentence(book_1_chapters, book_1_sent)\n",
        "\n",
        "book_2_sent = []\n",
        "tokenizeSentence(book_2_chapters, book_2_sent)\n",
        "\n",
        "book_3_sent = []\n",
        "tokenizeSentence(book_3_chapters, book_3_sent)\n",
        "\n",
        "book_4_sent = []\n",
        "tokenizeSentence(book_4_chapters, book_4_sent)\n",
        "\n",
        "book_5_sent = []\n",
        "tokenizeSentence(book_5_chapters, book_5_sent)\n",
        "\n",
        "book_6_sent = []\n",
        "tokenizeSentence(book_6_chapters, book_6_sent)\n",
        "\n",
        "book_7_sent = []\n",
        "tokenizeSentence(book_7_chapters, book_7_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d63d079",
      "metadata": {
        "id": "5d63d079"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe to store all sentences from 7 books\n",
        "def createDF(sent_list, df, num):\n",
        "    df = pd.DataFrame()\n",
        "    df = pd.DataFrame(sent_list, columns = [\"sentence\"]) \n",
        "    df[\"book\"] = num\n",
        "    return df\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df1 = createDF(book_1_sent,df,1)\n",
        "df2 = createDF(book_2_sent,df,2)\n",
        "df3 = createDF(book_3_sent,df,3)\n",
        "df4 = createDF(book_4_sent,df,4)\n",
        "df5 = createDF(book_5_sent,df,5)\n",
        "df6 = createDF(book_6_sent,df,6)\n",
        "df7 = createDF(book_7_sent,df,7)\n",
        "\n",
        "df=pd.concat([df1,df2,df3,df4,df5,df6,df7],axis=0)\n",
        "# df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a4b792b",
      "metadata": {
        "id": "2a4b792b"
      },
      "source": [
        "<a id=\"3\"></a>\n",
        "\n",
        "<h1 style=\"background-color:#740001;font-family:newtimeroman;font-size:225%;text-align:center;border-radius:15px 50px;\"><span style=\"color:#EEBA30\">Preprocessing</span></h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9a692d7",
      "metadata": {
        "id": "b9a692d7"
      },
      "outputs": [],
      "source": [
        "# Stopwords for wordcloud for topic modeling\n",
        "\n",
        "stpwrd = nltk.corpus.stopwords.words(\"english\")\n",
        "custom_stop_words = [\"harry\",\"potter\",\"hermione\",\"ron\",\"j\",\"k\",\"rowling\",\"s\",\"t\",\"said\",\"page\",\"professor\",\"know\",\n",
        "                     \"back\",\"i\",\"j.k\",\"like\",\"could\",'would',\"philosophers\",\"stone\",\"chamber\",\"secrets\",\"prisoner\",\n",
        "                     \"azbakan\",\"goblet\",\"fire\",\"order\",\"phoenix\",\"half\",\"blood\",\"prince\",\"deathly\",\"hallows\",\n",
        "                     \"looked\",\"one\",\"got\",\"get\",\"see\",\"going\",\"go\",\"told\",\"look\",\"looking\",\"tell\",\"saw\",\"think\",\n",
        "                     \"around\",\"though\",\"even\",\"still\"]\n",
        "all_stpwrd = stpwrd + custom_stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b916953",
      "metadata": {
        "id": "4b916953"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Lowercase text\n",
        "    text = re.sub(r\"[!#$+-@\\']/g\",\"\", text) # remove special characters and punctuations\n",
        "    text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)  # Remove punctuation\n",
        "    text = re.sub(\"<[^>]*>\", \" \", text) # Replace HTML tags with space - this needs to be before removing special characters\n",
        "    text = re.sub(r\"\\W\",\" \",text) # Replace non-alphanumerics with space\n",
        "    text = re.sub(r\"\\s+[a-z]\\s+\",\" \", text) # Remove single character\n",
        "    text = re.sub(r\"\\s+\",\" \",text)  # Replace multiple spaces with one space\n",
        "#     text = re.sub(r'(.)\\1{3,}',r'\\1', text) # Remove repeated characters like hellooo \n",
        "#     text = re.sub(r\"[^A-Za-z0-9\\s]+\", \" \", text) # Replace special characters and punctuation with space\n",
        "#     text = \" \".join(text.split())  # Remove extra spaces, tabs, and new lines\n",
        "    text = remove_stopwords(text)\n",
        "    text = lemmatize(text)\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    lst = []\n",
        "    for token in text.split():\n",
        "        if token not in stpwrd:\n",
        "            lst.append(token)\n",
        "    return \" \".join(lst)\n",
        "\n",
        "def lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    lemmas = \" \".join([token.lemma_ for token in doc])\n",
        "    return lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f1b0b9",
      "metadata": {
        "id": "23f1b0b9"
      },
      "outputs": [],
      "source": [
        "# def chapter_prep(book):\n",
        "#     book_prep=[]\n",
        "#     for sentence in book:\n",
        "#         sentence = preprocess_text(sentence)\n",
        "#         sentence=remove_stopwords(sentence)\n",
        "#         sentence=lemmatize(sentence)\n",
        "#         book_prep.append(sentence)\n",
        "#     return book_prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d42d5e",
      "metadata": {
        "id": "38d42d5e"
      },
      "outputs": [],
      "source": [
        "# book1_chapters_prep=chapter_prep(book1chapters)\n",
        "# book1=s_replace(book1_chapters_prep)\n",
        "# book2_chapters_prep=chapter_prep(book2chapters)\n",
        "# book2=s_replace(book2_chapters_prep)\n",
        "# book3_chapters_prep=chapter_prep(book3chapters)\n",
        "# book3=s_replace(book3_chapters_prep)\n",
        "# book4_chapters_prep=chapter_prep(book4chapters)\n",
        "# book4=s_replace(book4_chapters_prep)\n",
        "# book5_chapters_prep=chapter_prep(book5chapters)\n",
        "# book5=s_replace(book5_chapters_prep)\n",
        "# book6_chapters_prep=chapter_prep(book6chapters)\n",
        "# book6=s_replace(book6_chapters_prep)\n",
        "# book7_chapters_prep=chapter_prep(book7chapters)\n",
        "# book7=s_replace(book7_chapters_prep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3218720",
      "metadata": {
        "id": "e3218720"
      },
      "outputs": [],
      "source": [
        "book_1_prep = preprocess_text(book_1)\n",
        "book_2_prep = preprocess_text(book_2)\n",
        "book_3_prep = preprocess_text(book_3)\n",
        "book_4_prep = preprocess_text(book_4)\n",
        "book_5_prep = preprocess_text(book_5)\n",
        "book_6_prep = preprocess_text(book_6)\n",
        "book_7_prep = preprocess_text(book_7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9792f153",
      "metadata": {
        "id": "9792f153"
      },
      "source": [
        "<a id=\"4\"></a>\n",
        "\n",
        "<h1 style=\"background-color:#740001;font-family:newtimeroman;font-size:225%;text-align:center;border-radius:15px 50px;\"><span style=\"color:#EEBA30\">EDA</span></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd146ee8",
      "metadata": {
        "id": "bd146ee8"
      },
      "source": [
        "<a id=\"4.1\"></a>\n",
        "\n",
        "<h1 style=\"background-color:#1A472A;font-family:newtimeroman;font-size:200%;text-align:center;border-radius:15px 50px;\"><span style=\"color:#DCDCDC\">General Statistics</span></h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c050df59",
      "metadata": {
        "id": "c050df59"
      },
      "outputs": [],
      "source": [
        "book_title = [\"Book 1\" , \"Book 2\", \"Book 3\", \"Book 4\" , \"Book 5\", \"Book 6\" , \"Book 7\"]\n",
        "\n",
        "# book_title = [\"Harry Potter and the Sorcerer's Stone\",\n",
        "#               \"Harry Potter and the Chamber of Secrets\",\n",
        "#               \"Harry Potter and the Prisoner of Azkaban\",\n",
        "#               \"Harry Potter and the Goblet of Fire\",\n",
        "#               \"Harry Potter and the Order of the Phoenix\",\n",
        "#               \"Harry Potter and the Half-Blood Prince\",\n",
        "#               \"Harry Potter and the Deathly Hallows\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "445933f3",
      "metadata": {
        "scrolled": true,
        "id": "445933f3"
      },
      "outputs": [],
      "source": [
        "# Get number of chapters per book\n",
        "countChap = {book_title[0]: len(book_1_chapters),\n",
        "             book_title[1]: len(book_2_chapters),\n",
        "             book_title[2]: len(book_3_chapters),\n",
        "             book_title[3]: len(book_4_chapters),\n",
        "             book_title[4]: len(book_5_chapters),\n",
        "             book_title[5]: len(book_6_chapters),\n",
        "             book_title[6]: len(book_7_chapters)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35d07e55",
      "metadata": {
        "id": "35d07e55"
      },
      "outputs": [],
      "source": [
        "# Get number of sentences per book\n",
        "countSent = {book_title[0]: len(book_1_sent),\n",
        "             book_title[1]: len(book_2_sent),\n",
        "             book_title[2]: len(book_3_sent),\n",
        "             book_title[3]: len(book_4_sent),\n",
        "             book_title[4]: len(book_5_sent),\n",
        "             book_title[5]: len(book_6_sent),\n",
        "             book_title[6]: len(book_7_sent)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c99258a",
      "metadata": {
        "id": "7c99258a"
      },
      "outputs": [],
      "source": [
        "# Get number of words per book\n",
        "words1 = word_tokenize(book_1)\n",
        "words2 = word_tokenize(book_2)\n",
        "words3 = word_tokenize(book_3)\n",
        "words4 = word_tokenize(book_4)\n",
        "words5 = word_tokenize(book_5)\n",
        "words6 = word_tokenize(book_6)\n",
        "words7 = word_tokenize(book_7)\n",
        "\n",
        "countWord = {book_title[0]: len(book_1),\n",
        "             book_title[1]: len(book_2),\n",
        "             book_title[2]: len(book_3),\n",
        "             book_title[3]: len(book_4),\n",
        "             book_title[4]: len(book_5),\n",
        "             book_title[5]: len(book_6),\n",
        "             book_title[6]: len(book_7)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0331f92",
      "metadata": {
        "scrolled": false,
        "id": "e0331f92"
      },
      "outputs": [],
      "source": [
        "# Plot the statistics\n",
        "fig, ax = plt.subplots(1,3)\n",
        "sns.set(rc={'figure.figsize':(7,20)})\n",
        "plt.ticklabel_format(style='plain', axis='y')\n",
        "sns.barplot(x=list(countChap.keys()),y=list(countChap.values()),ax=ax[0]).set_title(\"Number of Chapters in Each Book\")\n",
        "sns.barplot(x=list(countSent.keys()),y=list(countSent.values()),ax=ax[1]).set_title(\"Number of Sentences in Each Book\")\n",
        "sns.barplot(x=list(countWord.keys()),y=list(countWord.values()),ax=ax[2]).set_title(\"Number of Words in Each Book\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4ea4348",
      "metadata": {
        "id": "c4ea4348"
      },
      "source": [
        "<a id=\"4.1\"></a>\n",
        "\n",
        "<h1 style=\"background-color:#1A472A;font-family:newtimeroman;font-size:200%;text-align:center;border-radius:15px 50px;\"><span style=\"color:#DCDCDC\">N-Grams</span></h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06cb8809",
      "metadata": {
        "id": "06cb8809"
      },
      "outputs": [],
      "source": [
        "def compute_ngrams(sequence, n):\n",
        "    return list(\n",
        "            zip(*(sequence[index:] \n",
        "                     for index in range(n)))\n",
        "    )\n",
        "\n",
        "def get_top_ngrams(corpus, ngram_val, limit):\n",
        "    tokens = nltk.word_tokenize(corpus)\n",
        "    ngrams = compute_ngrams(tokens, ngram_val)\n",
        "    ngrams_freq_dist = nltk.FreqDist(ngrams)\n",
        "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(), \n",
        "                              key=itemgetter(1), reverse=True)\n",
        "    sorted_ngrams = sorted_ngrams_fd[0:limit]\n",
        "    sorted_ngrams = [(' '.join(text), freq) \n",
        "                     for text, freq in sorted_ngrams]\n",
        "\n",
        "    return sorted_ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ffae88f",
      "metadata": {
        "id": "9ffae88f"
      },
      "outputs": [],
      "source": [
        "get_top_ngrams(corpus=book_1_prep, ngram_val=2,limit=10)\n",
        "# get_top_ngrams(corpus=book_1_prep, ngram_val=3,limit=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "319e842e",
      "metadata": {
        "id": "319e842e"
      },
      "outputs": [],
      "source": [
        "get_top_ngrams(corpus=book_2_prep, ngram_val=2,limit=10)\n",
        "# get_top_ngrams(corpus=book_2_prep, ngram_val=3,limit=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38e0c65f",
      "metadata": {
        "scrolled": true,
        "id": "38e0c65f"
      },
      "outputs": [],
      "source": [
        "get_top_ngrams(corpus=book_3_prep, ngram_val=2,limit=10)\n",
        "# get_top_ngrams(corpus=book_3_prep, ngram_val=3,limit=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b8c4a26",
      "metadata": {
        "id": "0b8c4a26"
      },
      "outputs": [],
      "source": [
        "get_top_ngrams(corpus=book_4_prep, ngram_val=2,limit=10)\n",
        "# get_top_ngrams(corpus=book_4_prep, ngram_val=3,limit=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d091db6",
      "metadata": {
        "id": "5d091db6"
      },
      "outputs": [],
      "source": [
        "get_top_ngrams(corpus=book_5_prep, ngram_val=2,limit=10)\n",
        "# get_top_ngrams(corpus=book_5_prep, ngram_val=3,limit=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81aee318",
      "metadata": {
        "id": "81aee318"
      },
      "outputs": [],
      "source": [
        "get_top_ngrams(corpus=book_6_prep, ngram_val=2,limit=10)\n",
        "# get_top_ngrams(corpus=book_6_prep, ngram_val=3,limit=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "582601d6",
      "metadata": {
        "id": "582601d6"
      },
      "outputs": [],
      "source": [
        "get_top_ngrams(corpus=book_7_prep, ngram_val=2,limit=10)\n",
        "# get_top_ngrams(corpus=book_7_prep, ngram_val=3,limit=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29696af8",
      "metadata": {
        "id": "29696af8"
      },
      "source": [
        "<a id=\"4.2\"></a>\n",
        "\n",
        "<h1 style=\"background-color:#1A472A;font-family:newtimeroman;font-size:200%;text-align:center;border-radius:15px 50px;\"><span style=\"color:#DCDCDC\">Character Appearance Frequency</span></h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1443fc9a",
      "metadata": {
        "id": "1443fc9a"
      },
      "outputs": [],
      "source": [
        "# Get character's first names\n",
        "characters_list[\"first_name\"] = characters_list['name'].apply(lambda s: s.split()[0].lower())\n",
        "\n",
        "# Get character's last names\n",
        "characters_list[\"last_name\"] = characters_list['name'].apply(lambda s: s.split()[-1].lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b36485",
      "metadata": {
        "id": "25b36485"
      },
      "outputs": [],
      "source": [
        "# Convert all tokenized words to lowercase\n",
        "words1 = [x.lower() for x in words1]\n",
        "words2 = [x.lower() for x in words2]\n",
        "words3 = [x.lower() for x in words3]\n",
        "words4 = [x.lower() for x in words4]\n",
        "words5 = [x.lower() for x in words5]\n",
        "words6 = [x.lower() for x in words6]\n",
        "words7 = [x.lower() for x in words7]\n",
        "\n",
        "# Creat a count dictionary for each book\n",
        "countDict1 = Counter(words1)\n",
        "countDict2 = Counter(words2)\n",
        "countDict3 = Counter(words3)\n",
        "countDict4 = Counter(words4)\n",
        "countDict5 = Counter(words5)\n",
        "countDict6 = Counter(words6)\n",
        "countDict7 = Counter(words7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5479c2de",
      "metadata": {
        "id": "5479c2de"
      },
      "outputs": [],
      "source": [
        "def charFreq(countDict, num):\n",
        "    bowDF = pd.DataFrame(countDict.items()).rename(columns={0:'word',1:'count'})\n",
        "    \n",
        "    fig, ax = plt.subplots(1,2)\n",
        "    plt.figure(figsize=(15,8))\n",
        "    \n",
        "    fn = bowDF[bowDF['word'].isin(characters_list[\"first_name\"])].sort_values(by='count',ascending=False).iloc[0:10]\n",
        "    sns.barplot(x = fn['word'],y = fn['count'], ax=ax[0]).set_title(f\"Character Appearance in Book {num} by First Name\")\n",
        "\n",
        "    ln = bowDF[bowDF['word'].isin(characters_list[\"last_name\"])].sort_values(by='count',ascending=False).iloc[0:10]\n",
        "    sns.barplot(x = ln['word'],y = ln['count'], ax=ax[1]).set_title(f\"Character Appearance in Book {num} by Last Name\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0814db2d",
      "metadata": {
        "scrolled": false,
        "id": "0814db2d"
      },
      "outputs": [],
      "source": [
        "charFreq(countDict1,1)\n",
        "charFreq(countDict2,2)\n",
        "charFreq(countDict3,3)\n",
        "charFreq(countDict4,4)\n",
        "charFreq(countDict5,5)\n",
        "charFreq(countDict5,5)\n",
        "charFreq(countDict6,6)\n",
        "charFreq(countDict7,7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4112bdde",
      "metadata": {
        "id": "4112bdde"
      },
      "source": [
        "<a id=\"5\"></a>\n",
        "\n",
        "<h1 style=\"background-color:#740001;font-family:newtimeroman;font-size:225%;text-align:center;border-radius:15px 50px;\"><span style=\"color:#EEBA30\">Sentiment Analysis</span></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98cf5af5",
      "metadata": {
        "id": "98cf5af5"
      },
      "source": [
        "<a id=\"5.1\"></a>\n",
        "\n",
        "<h1 style=\"background-color:#1A472A;font-family:newtimeroman;font-size:200%;text-align:center;border-radius:15px 50px;\"><span style=\"color:#DCDCDC\">Sentiment Analysis For Each Sentence</span></h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89eeecd7",
      "metadata": {
        "id": "89eeecd7"
      },
      "outputs": [],
      "source": [
        "sentiment_model=SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14572009",
      "metadata": {
        "scrolled": true,
        "id": "14572009"
      },
      "outputs": [],
      "source": [
        "def sent_by_sent(sent_list):\n",
        "    for sentence in sent_list:\n",
        "        scores = sentiment_model.polarity_scores(sentence)\n",
        "        print(sentence)\n",
        "        print(\"negative: {: <15} neutral: {: <15} positive: {: <15} compound: {}\\n\".format(\n",
        "            scores['neg'], scores['neu'], scores['pos'], scores['compound']\n",
        "        ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff67714",
      "metadata": {
        "id": "0ff67714"
      },
      "outputs": [],
      "source": [
        "sent_by_sent(book_1_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eccd975b",
      "metadata": {
        "id": "eccd975b"
      },
      "outputs": [],
      "source": [
        "sent_by_sent(book_2_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e09e93e7",
      "metadata": {
        "id": "e09e93e7"
      },
      "outputs": [],
      "source": [
        "sent_by_sent(book_3_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9debb8c5",
      "metadata": {
        "id": "9debb8c5"
      },
      "outputs": [],
      "source": [
        "sent_by_sent(book_4_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "515b432b",
      "metadata": {
        "id": "515b432b"
      },
      "outputs": [],
      "source": [
        "sent_by_sent(book_5_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07f111bf",
      "metadata": {
        "id": "07f111bf"
      },
      "outputs": [],
      "source": [
        "sent_by_sent(book_6_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa999c0e",
      "metadata": {
        "id": "fa999c0e"
      },
      "outputs": [],
      "source": [
        "sent_by_sent(book_7_sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b717dc5d",
      "metadata": {
        "id": "b717dc5d"
      },
      "source": [
        "<a id=\"5.2\"></a>\n",
        "\n",
        "<h1 style=\"background-color:#1A472A;font-family:newtimeroman;font-size:200%;text-align:center;border-radius:15px 50px;\"><span style=\"color:#DCDCDC\">Sentiment Analysis For Each Chapter</span></h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f26124ab",
      "metadata": {
        "id": "f26124ab"
      },
      "outputs": [],
      "source": [
        "# fig, ax = plt.subplots(2,4)\n",
        "# plt.rcParams.update({\"figure.figsize\":(10,7), \"figure.dpi\":100})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c66cea9c",
      "metadata": {
        "id": "c66cea9c"
      },
      "outputs": [],
      "source": [
        "# Store sentiment scores for each chapter in a list\n",
        "def chapter_score(book_chapters):\n",
        "    chapter_scores = []\n",
        "    for chapter in book_chapters:\n",
        "        sentences = sent_tokenize(chapter)\n",
        "\n",
        "        # Calculate average score, we will first store scores for all sentences\n",
        "        all_scores = []    \n",
        "        for sentence in sentences:\n",
        "            scores = sentiment_model.polarity_scores(sentence)\n",
        "            compound_score = scores['compound']\n",
        "            all_scores.append(compound_score)\n",
        "\n",
        "        # Average is calculated as sum of all scores divided by number of scores\n",
        "        chapter_score = sum(all_scores) / len(all_scores)\n",
        "        chapter_scores.append(chapter_score)\n",
        "    return chapter_scores\n",
        "\n",
        "# Plot sentiment scores for each chapter by book\n",
        "def plot_sent_scores(chapter_names, chapter_scores, i):\n",
        "    plt.figure(figsize=(15, 8))    \n",
        "    pos = list(range(len(chapter_scores)))   \n",
        "    plt.plot(pos, list(chapter_scores))   \n",
        "    plt.xticks(pos, chapter_names, rotation=90)\n",
        "    plt.title(f\"Sentiment Throughout Book {i} By Chapters\", fontsize=18)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a82ffa2a",
      "metadata": {
        "scrolled": false,
        "id": "a82ffa2a"
      },
      "outputs": [],
      "source": [
        "# Plot sentiment score for book 1\n",
        "book_1_scores = chapter_score(book_1_chapters)\n",
        "plot_sent_scores(book_1_chapter_names, book_1_scores,1)\n",
        "\n",
        "# Plot sentiment score for book 2\n",
        "book_2_scores = chapter_score(book_2_chapters)\n",
        "plot_sent_scores(book_2_chapter_names, book_2_scores,2)\n",
        "\n",
        "# Plot sentiment score for book 3\n",
        "book_3_scores = chapter_score(book_3_chapters)\n",
        "plot_sent_scores(book_3_chapter_names, book_3_scores,3)\n",
        "\n",
        "# Plot sentiment score for book 4\n",
        "book_4_scores = chapter_score(book_4_chapters)\n",
        "plot_sent_scores(book_4_chapter_names, book_4_scores,4)\n",
        "\n",
        "# Plot sentiment score for book 5\n",
        "book_5_scores = chapter_score(book_5_chapters)\n",
        "plot_sent_scores(book_5_chapter_names, book_5_scores,5)\n",
        "\n",
        "# Plot sentiment score for book 6\n",
        "book_6_scores = chapter_score(book_6_chapters)\n",
        "plot_sent_scores(book_6_chapter_names, book_6_scores,6)\n",
        "\n",
        "# Plot sentiment score for book 7\n",
        "book_7_scores = chapter_score(book_7_chapters)\n",
        "plot_sent_scores(book_7_chapter_names, book_7_scores,7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0493ac3",
      "metadata": {
        "id": "c0493ac3"
      },
      "source": [
        "<a id=\"5.3\"></a>\n",
        "\n",
        "<h1 style=\"background-color:#1A472A;font-family:newtimeroman;font-size:200%;text-align:center;border-radius:15px 50px;\"><span style=\"color:#DCDCDC\">Sentiment Analysis For Entire Series</span></h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3d765a3",
      "metadata": {
        "id": "f3d765a3"
      },
      "outputs": [],
      "source": [
        "# Store sentiment scores for all chapters in a list\n",
        "all_chapters, all_names = load_all_chapters()\n",
        "\n",
        "all_chapter_scores = []\n",
        "\n",
        "for book_chapters in all_chapters:\n",
        "    chapter_scores = []\n",
        "    for chapter in book_chapters:\n",
        "        sentences = sent_tokenize(chapter)\n",
        "        all_scores = []    \n",
        "        for sentence in sentences:\n",
        "            scores = sentiment_model.polarity_scores(sentence)\n",
        "            compound_score = scores['compound']\n",
        "            all_scores.append(compound_score)\n",
        "\n",
        "        # Average is calculated as sum of all scores divided by number of scores\n",
        "        chapter_score = sum(all_scores) / len(all_scores)\n",
        "        chapter_scores.append(chapter_score)\n",
        "        \n",
        "    all_chapter_scores.append(chapter_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59383839",
      "metadata": {
        "id": "59383839"
      },
      "outputs": [],
      "source": [
        "# Plot sentiment scores for all chapters all books\n",
        "def plot_all_book_scores(all_names, all_scores):\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    c = ['blue', 'green', 'yellow', 'red', 'black', 'orange', 'purple']\n",
        "    \n",
        "    previous = 0\n",
        "    \n",
        "    for idx, scores in enumerate(all_scores):\n",
        "        pos = [i + previous for i in range(len(scores))]\n",
        "        previous += len(scores)\n",
        "        plt.plot(pos, scores, color=c[idx])\n",
        "\n",
        "    plt.xlim((-0.5, previous + 0.5))\n",
        "    xticks = [n for ns in all_names for n in ns]\n",
        "    plt.xticks(list(range(len(xticks))), xticks, rotation=90, fontsize=6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('all_chapters.pdf')\n",
        "    plt.show()\n",
        "    \n",
        "plot_all_book_scores(all_names, all_chapter_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d7a99bb",
      "metadata": {
        "id": "5d7a99bb"
      },
      "outputs": [],
      "source": [
        "def sentiment_rating(chapter_sentences):\n",
        "    polar=sentiment_model.polarity_scores(chapter_sentences)\n",
        "    comp=polar['compound']\n",
        "    return comp\n",
        "    \n",
        "df['Vader_sentiment']=df['sentence'].apply(sentiment_rating)\n",
        "\n",
        "def sentiment_polarity(comp):\n",
        "    if comp < 0:\n",
        "        return  'negative'\n",
        "    elif comp == 0:\n",
        "        return  'neutral'\n",
        "    else: \n",
        "        return 'positive'\n",
        "df['Vader_polarity']=df['Vader_sentiment'].apply(sentiment_polarity)\n",
        "def star_rating(compound):\n",
        "    if compound <-0.25:\n",
        "        return '1'\n",
        "    elif compound > -0.25 and compound < 0:\n",
        "        return '2'\n",
        "    elif compound >=0 and compound < 0.25:\n",
        "        return '3'\n",
        "    elif compound > 0.25 and compound < 0.75:\n",
        "        return '4'\n",
        "    else: \n",
        "        return '5'\n",
        "df['Vader_star']=df['Vader_sentiment'].apply(star_rating)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b249666d",
      "metadata": {
        "id": "b249666d"
      },
      "outputs": [],
      "source": [
        "def sentiment_analysis(text):\n",
        "\n",
        "    # Create a function to get the subjectivity\n",
        "    def getSubjectivity(text):\n",
        "        return TextBlob(text).sentiment.subjectivity\n",
        "  \n",
        "    # Create a function to get the polarity\n",
        "    def getPolarity(text):\n",
        "        return TextBlob(text).sentiment.polarity\n",
        "  \n",
        "    # Create two new columns Subjectivity & Polarity\n",
        "    df['TextBlob_Subjectivity'] = text.apply(getSubjectivity)\n",
        "    df['TextBlob_Polarity'] = text.apply(getPolarity)\n",
        " \n",
        "    #Transform scores to text-based sentiment labels\n",
        "    def getAnalysis(score):\n",
        "        if score < 0:\n",
        "            return 'negative'\n",
        "        elif score == 0:\n",
        "            return 'neutral'\n",
        "        else:\n",
        "            return 'positive'\n",
        "    \n",
        "    df['TextBlob_Analysis'] = df['TextBlob_Polarity'].apply(getAnalysis)\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71fe7110",
      "metadata": {
        "scrolled": true,
        "id": "71fe7110"
      },
      "outputs": [],
      "source": [
        "sentiment_analysis(df['sentence'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e4a1f0d",
      "metadata": {
        "id": "8e4a1f0d"
      },
      "source": [
        "<a id=\"6\"></a>\n",
        "\n",
        "<h1 style=\"background-color:#740001;font-family:newtimeroman;font-size:225%;text-align:center;border-radius:15px 50px;\"><span style=\"color:#EEBA30\">LDA Topic Modeling</span></h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1b965b",
      "metadata": {
        "id": "1b1b965b"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words=all_stpwrd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a37313c",
      "metadata": {
        "id": "0a37313c"
      },
      "outputs": [],
      "source": [
        "dtm = cv.fit_transform(book_1_chapters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df168c4c",
      "metadata": {
        "scrolled": false,
        "id": "df168c4c"
      },
      "outputs": [],
      "source": [
        "LDA = LatentDirichletAllocation(n_components=len(book_1_chapters),random_state=42)\n",
        "LDA.fit(dtm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20225357",
      "metadata": {
        "id": "20225357"
      },
      "outputs": [],
      "source": [
        "for index,topic in enumerate(LDA.components_):\n",
        "    print(f'TOP 20 WORDS FOR TOPIC #{index+1}')\n",
        "    print([cv.get_feature_names_out()[i] for i in topic.argsort()[-20:]])\n",
        "    print('\\n')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "HP_Sentiment_Analysis.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}